{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/zoo_optimizer.py:73: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "+------+-----+------+-----+--------------------+\n",
      "|   uid|  mid|labels|count|            features|\n",
      "+------+-----+------+-----+--------------------+\n",
      "|   4.0|  9.0|   0.0|   12|    [4.0, 9.0, 12.0]|\n",
      "|1574.0|  4.0|   0.0|    0|  [1574.0, 4.0, 0.0]|\n",
      "|2107.0|200.0|   0.0|    0|[2107.0, 200.0, 0.0]|\n",
      "|1771.0|  7.0|   0.0|    1|  [1771.0, 7.0, 1.0]|\n",
      "|1222.0| 40.0|   0.0|    0| [1222.0, 40.0, 0.0]|\n",
      "+------+-----+------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None,)]            0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None,)]            0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50)           500050      tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50)           10050       tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 50)           0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 50)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50)           5050        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            102         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 515,252\n",
      "Trainable params: 515,252\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_dataset.py:191: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_dataset.py:209: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_dataset.py:210: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/util/tf.py:37: The name tf.unsorted_segment_sum is deprecated. Please use tf.math.unsorted_segment_sum instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:659: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n",
      "creating: createTFParkSampleToMiniBatch\n",
      "creating: createZooKerasSparseCategoricalCrossEntropy\n",
      "creating: createLoss\n",
      "creating: createZooKerasSparseCategoricalAccuracy\n",
      "creating: createFakeOptimMethod\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:295: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:159: The name tf.is_numeric_tensor is deprecated. Please use tf.debugging.is_numeric_tensor instead.\n",
      "\n",
      "creating: createTFValidationMethod\n",
      "creating: createTFValidationMethod\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:187: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:202: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:243: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:276: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:285: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "creating: createTFTrainingHelper\n",
      "creating: createIdentityCriterion\n",
      "creating: createTFParkSampleToMiniBatch\n",
      "creating: createTFParkSampleToMiniBatch\n",
      "creating: createEstimator\n",
      "creating: createMaxEpoch\n",
      "creating: createEveryEpoch\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp0addx9vm/model\n",
      "['loss', 'acc']\n",
      "WARNING:tensorflow:From /tmp/spark-69561b55-f930-466d-b413-e25c4e736166/userFiles-12fd88e0-e0df-4e56-857e-345e82e60fe8/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tfnet.py:275: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "WARNING:tensorflow:Issue encountered when serializing features:0.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'DataFrameDataset' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing labels:0.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'DataFrameDataset' object has no attribute 'name'\n",
      "INFO:tensorflow:SavedModel written to: /tmp/tmpv1ry9oin/saved_model.pb\n",
      "+------+----+------+-----+-------------------+--------------------+\n",
      "|   uid| mid|labels|count|           features|          prediction|\n",
      "+------+----+------+-----+-------------------+--------------------+\n",
      "|   4.0| 9.0|   0.0|   15|   [4.0, 9.0, 15.0]|[-0.1236562877893...|\n",
      "| 853.0| 6.0|   0.0|    0|  [853.0, 6.0, 0.0]|[-0.0822267830371...|\n",
      "|1771.0| 7.0|   0.0|    0| [1771.0, 7.0, 0.0]|[-0.1819527447223...|\n",
      "|2887.0| 6.0|   0.0|    1| [2887.0, 6.0, 1.0]|[-0.1470358073711...|\n",
      "|2429.0|27.0|   0.0|    1|[2429.0, 27.0, 1.0]|[-0.1620292216539...|\n",
      "+------+----+------+-----+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+----+------+-----+-------------------+--------------------+-----------+\n",
      "|   uid| mid|labels|count|           features|          prediction|prediction2|\n",
      "+------+----+------+-----+-------------------+--------------------+-----------+\n",
      "|   4.0| 9.0|   0.0|   15|   [4.0, 9.0, 15.0]|[-0.1236562877893...|        0.0|\n",
      "| 853.0| 6.0|   0.0|    0|  [853.0, 6.0, 0.0]|[-0.0822267830371...|        0.0|\n",
      "|1771.0| 7.0|   0.0|    0| [1771.0, 7.0, 0.0]|[-0.1819527447223...|        0.0|\n",
      "|2887.0| 6.0|   0.0|    1| [2887.0, 6.0, 1.0]|[-0.1470358073711...|        0.0|\n",
      "|2429.0|27.0|   0.0|    1|[2429.0, 27.0, 1.0]|[-0.1620292216539...|        0.0|\n",
      "|2041.0| 2.0|   0.0|    0| [2041.0, 2.0, 0.0]|[-0.1466202288866...|        0.0|\n",
      "|3678.0|11.0|   0.0|    0|[3678.0, 11.0, 0.0]|[-0.1147139966487...|        0.0|\n",
      "|3176.0|89.0|   0.0|    1|[3176.0, 89.0, 1.0]|[-0.1561386585235...|        0.0|\n",
      "|2553.0|39.0|   0.0|    2|[2553.0, 39.0, 2.0]|[-0.1292952597141...|        0.0|\n",
      "| 457.0| 3.0|   0.0|    3|  [457.0, 3.0, 3.0]|[-0.1701820939779...|        0.0|\n",
      "+------+----+------+-----+-------------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import argparse\n",
    "import importlib\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "import tensorflow as tf\n",
    "\n",
    "from bigdl.optim.optimizer import *\n",
    "from zoo import init_nncontext, init_spark_conf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, col, udf, lit\n",
    "from pyspark.sql.types import FloatType,DoubleType,ArrayType\n",
    "from zoo.orca.learn.tf.estimator import Estimator\n",
    "from zoo.orca.learn.tf.utils import convert_predict_to_dataframe\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "if os.path.exists('jobs.zip'):\n",
    "    sys.path.insert(0, 'jobs.zip')\n",
    "else:\n",
    "    sys.path.insert(0, './jobs')\n",
    "\n",
    "# load dynamic module\n",
    "ncf_features = importlib.import_module(\"jobs.ncf_features\")\n",
    "ncf_model = importlib.import_module(\"jobs.ncf_model\")\n",
    "\n",
    "\n",
    "__author__ = 'suqiang.song@mastercard.com'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app_name = \"NCF_DL\"\n",
    "    data_source_path = \"/opt/work/data/pcard.csv\"\n",
    "    model_file_name = app_name + '.h5'\n",
    "    save_model_dir = \"/opt/work/model/\" + model_file_name\n",
    "    u_limit = 10000\n",
    "    m_limit = 200\n",
    "    neg_rate = 5\n",
    "    sliding_length = 1\n",
    "    u_output = 50\n",
    "    m_output = 50\n",
    "    max_epoch = 5\n",
    "    batch_size = 400\n",
    "    predict_output_path = \"/opt/work/output/\"\n",
    "    log_dir = \"/opt/work/logs/\"\n",
    "    train_start = \"201307\"\n",
    "    train_end = \"201401\"\n",
    "    validation_start = \"201402\"\n",
    "    validation_end = \"201403\"\n",
    "    test_start = \"201403\"\n",
    "    test_end = \"201404\"\n",
    "    inference_start = \"201405\"\n",
    "    inference_end = \"201406\"\n",
    "    \n",
    "    \n",
    "    sparkConf = init_spark_conf()\n",
    "    sc = init_nncontext(sparkConf)\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    start = time.time()\n",
    "    uDF, mDF, tDF = ncf_features.load_csv(spark,data_source_path,u_limit,m_limit)\n",
    "    trainingDF = ncf_features.genData(tDF,sc,spark,train_start, train_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    #trainingDF.show(5)\n",
    "    validationDF = ncf_features.genData(tDF,sc,spark,validation_start, validation_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    validationDF.show(5)\n",
    "    testDF = ncf_features.genData(tDF,sc,spark,test_start,test_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    #testDF.show(5)\n",
    "    inferenceDF = ncf_features.genData(tDF,sc,spark,inference_start,inference_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    #inferenceDF.show(5)\n",
    "\n",
    "    model = ncf_model.getKerasModel(u_limit,m_limit,u_output,m_output,log_dir)\n",
    "    est = Estimator.from_keras(model,model_dir=log_dir)\n",
    "    est.fit(data=trainingDF,batch_size=batch_size,epochs=max_epoch,feature_cols=['features'],labels_cols=['labels'],validation_data=validationDF)\n",
    "    # save the model\n",
    "    est.save_keras_model(save_model_dir)\n",
    "    # metrics ,result and save model\n",
    "    print(model.metrics_names)\n",
    "    #Orca the predict function supports native spark data frame ! Just need to tell batch_size and feature_cols\n",
    "    prediction_df = est.predict(inferenceDF, batch_size=batch_size, feature_cols=['features'])\n",
    "    prediction_df.show(5)\n",
    "    score_udf = udf(lambda pred: 0.0 if pred[0] > pred[1] else 1.0, FloatType())\n",
    "    prediction_df = prediction_df.withColumn('prediction2', score_udf('prediction'))\n",
    "    prediction_df.show(10)\n",
    "    # Save Table\n",
    "    #prediction_final_df.write.mode('overwrite').parquet(predict_output_path)\n",
    "    prediction_df.select('uid','mid','prediction2').write.mode('overwrite').parquet(predict_output_path)\n",
    "    #prediction_df.select('uid','mid','prediction2').write.mode('overwrite').format(\"csv\").save(predict_output_path)\n",
    "    #user_join_df = prediction_df.join(uDF, on=['uid'], how='inner')\n",
    "    #prediction_final_df = user_join_df.join(mDF, on=['mid'], how='inner').select('u','m','prediction').write.mode('overwrite').parquet(predict_output_path)\n",
    "    end = time.time()\n",
    "    print(\"Took time:\"+str((end-start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
