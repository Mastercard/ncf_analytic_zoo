{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import argparse\n",
    "import importlib\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "import tensorflow as tf\n",
    "\n",
    "from bigdl.optim.optimizer import *\n",
    "from zoo import init_nncontext, init_spark_conf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, col, udf, lit\n",
    "from pyspark.sql.types import FloatType,DoubleType,ArrayType\n",
    "from zoo.orca.learn.tf.estimator import Estimator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "if os.path.exists('jobs.zip'):\n",
    "    sys.path.insert(0, 'jobs.zip')\n",
    "else:\n",
    "    sys.path.insert(0, './jobs')\n",
    "\n",
    "# load dynamic module\n",
    "ncf_features = importlib.import_module(\"jobs.ncf_features\")\n",
    "ncf_model = importlib.import_module(\"jobs.ncf_model\")\n",
    "\n",
    "\n",
    "__author__ = 'suqiang.song@mastercard.com'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app_name = \"NCF_DL\"\n",
    "    data_source_path = \"/opt/work/data/pcard.csv\"\n",
    "    model_file_name = app_name + '.h5'\n",
    "    save_model_dir = \"/opt/work/model/\" + model_file_name\n",
    "    u_limit = 10000\n",
    "    m_limit = 200\n",
    "    neg_rate = 5\n",
    "    sliding_length = 1\n",
    "    u_output = 50\n",
    "    m_output = 50\n",
    "    max_epoch = 5\n",
    "    batch_size = 400\n",
    "    predict_output_path = \"/opt/work/output/\"\n",
    "    log_dir = \"/opt/work/logs/\"\n",
    "    train_start = \"201307\"\n",
    "    train_end = \"201401\"\n",
    "    validation_start = \"201402\"\n",
    "    validation_end = \"201403\"\n",
    "    test_start = \"201403\"\n",
    "    test_end = \"201404\"\n",
    "    inference_start = \"201405\"\n",
    "    inference_end = \"201406\"\n",
    "    \n",
    "    \n",
    "    sparkConf = init_spark_conf()\n",
    "    sc = init_nncontext(sparkConf)\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    start = time.time()\n",
    "    uDF, mDF, tDF = ncf_features.load_csv(spark,data_source_path,u_limit,m_limit)\n",
    "    trainingDF = ncf_features.genData(tDF,sc,spark,train_start, train_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    #trainingDF.show(5)\n",
    "    validationDF = ncf_features.genData(tDF,sc,spark,validation_start, validation_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    validationDF.show(5)\n",
    "    testDF = ncf_features.genData(tDF,sc,spark,test_start,test_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    #testDF.show(5)\n",
    "    inferenceDF = ncf_features.genData(tDF,sc,spark,inference_start,inference_end,neg_rate,sliding_length,u_limit,m_limit)\n",
    "    #inferenceDF.show(5)\n",
    "\n",
    "    model = ncf_model.getKerasModel(u_limit,m_limit,u_output,m_output,log_dir)\n",
    "    est = Estimator.from_keras(model,model_dir=log_dir)\n",
    "    est.fit(data=trainingDF,batch_size=batch_size,epochs=max_epoch,feature_cols=['features'],label_cols=['labels'],validation_data=validationDF)\n",
    "    # save the model\n",
    "    est.save_keras_model(save_model_dir)\n",
    "    # metrics ,result and save model\n",
    "    print(model.metrics_names)\n",
    "    #Orca the predict function supports native spark data frame ! Just need to tell batch_size and feature_cols\n",
    "    prediction_df = est.predict(inferenceDF, batch_size=batch_size, feature_cols=['features'])\n",
    "    prediction_df.show(5)\n",
    "    score_udf = udf(lambda pred: 0.0 if pred[0] > pred[1] else 1.0, FloatType())\n",
    "    prediction_df = prediction_df.withColumn('prediction2', score_udf('prediction'))\n",
    "    prediction_df.show(10)\n",
    "    # Save Table\n",
    "    #prediction_final_df.write.mode('overwrite').parquet(predict_output_path)\n",
    "    prediction_df.select('uid','mid','prediction2').write.mode('overwrite').parquet(predict_output_path)\n",
    "    #prediction_df.select('uid','mid','prediction2').write.mode('overwrite').format(\"csv\").save(predict_output_path)\n",
    "    #user_join_df = prediction_df.join(uDF, on=['uid'], how='inner')\n",
    "    #prediction_final_df = user_join_df.join(mDF, on=['mid'], how='inner').select('u','m','prediction').write.mode('overwrite').parquet(predict_output_path)\n",
    "    end = time.time()\n",
    "    print(\"Took time:\"+str((end-start)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
